{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0fca0-b098-4dfc-902d-a7c9ca4ea184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "importlib.reload(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0359e-9bbd-44db-acf2-11a0b2a8dca1",
   "metadata": {},
   "source": [
    "Here, we are importing the necessary libraries for text preprocessing and loading them. nltk and spacy are natural language processing libraries that help us tokenize and preprocess text. pandas is used for reading and writing data to and from spreadsheets. re is used for regular expressions, which can be used to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9608a-c234-4c5d-9afe-70fd4a44333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17533627-8490-48ee-9b72-d24d6f0c829d",
   "metadata": {},
   "source": [
    "This line loads the English language model from spacy and disables the named entity recognition and dependency parser components. This is done to make text preprocessing faster, as these components are not needed for text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7666a-1cd9-47c8-89f2-25656acf74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('output_past.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e155a0-5671-4dec-a773-f15c44b29b6e",
   "metadata": {},
   "source": [
    "This line reads the Excel file 'output_past.xlsx' into a Pandas dataframe called df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447e7a8-dc92-4e47-87b5-6d3591b6ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords_lower = [s.lower() for s in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735269e6-84a2-4f56-8d21-f7eb5533a92f",
   "metadata": {},
   "source": [
    "Here, we are importing a list of English stopwords from nltk. Stopwords are common words that are usually filtered out from text during preprocessing. We are also converting all the words in this list to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752185e2-1a54-4da5-ae5f-373eed4f70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(str_input): \n",
    "     words=[token.lemma_ for token in nlp(str_input) if not token.is_punct]\n",
    "  \n",
    "     words = [re.sub(r\"[^A-Za-z@]\", \"\", word) for word in words]\n",
    "     words = [re.sub(r\"\\S+com\", \"\", word) for word in words]\n",
    "     words = [re.sub(r\"\\S+@\\S+\", \"\", word) for word in words] \n",
    "     words = [word for word in words if word!=' ']\n",
    "     words = [word for word in words if len(word)!=0] \n",
    "      \n",
    "     words=[word.lower() for word in words if word.lower() not in stopwords_lower]\n",
    "        \n",
    "     string = \" \".join(words)\n",
    "     return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaa867-ec6b-49ba-8e68-19a5f2672547",
   "metadata": {},
   "source": [
    "This block of code defines a function called text_preprocessing that takes in a string str_input. The function first tokenizes the input using the nlp object we created earlier, and only keeps the lemmatized tokens (i.e., the base form of each word). It then removes any non-alphabetic characters except for the \"@\" symbol, which is kept to preserve email addresses. Next, it removes any words containing the string \"com\" and any email addresses. It then removes any words that are just spaces or have a length of zero. After that, it converts all the remaining words to lowercase and removes any stopwords we imported earlier. Finally, the function joins the remaining words back into a string separated by spaces and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d99ef9-25b3-4502-be66-79c8691b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].fillna('')\n",
    "df['news_cleaned'] = df['text'].apply(text_preprocessing)\n",
    "\n",
    "df.to_excel('output_past(preprocessed).xlsx', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196ef4c-adbc-472d-9a24-3e6f03f64108",
   "metadata": {},
   "source": [
    "These lines fill any missing values in the 'text' column of df with empty strings, then create a new column called 'news_cleaned' that contains the preprocessed text for each row. The apply() method is used to apply the text_preprocessing function to each row of the 'text' column. Finally, the preprocessed data is written to a new Excel file called 'output_past(preprocessed).xlsx' and the first five rows of the dataframe are printed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
